# LLM を使用したサービスのサンプル（Django + React + Ollama）

例外処理などは省き、最低限の実装で LLM を使用したサービスを構築するサンプルコードです。

## 使い方

手順を以下に示します。

```
1, リポジトリをローカルへクローン
2,　「環境構築」セクションを参照し、環境を構築
3, ブラウザで http://localhost:3333 にアクセス（ログイン認証は不要）
4、 質問を入力して送信
5、 暫くすると LLM が生成した回答が表示される

#　現在のプロンプト
- 年齢80歳の日本食料理人で設定しています。
- プロンプトの設定については「プロンプトの設定」セクションを参照してください。
```

## 環境構築

llm モデルは「llama3.1:8b」にしていますが、他のモデルも使用可能です。
「LLM モデル例」におすすめモデルを記載していますので、参考にしてみてください。

```
# dockerビルド
docker compose build --no-cache

# llmモデルのダウンロード（ここではllama3.1:8bをダウンロード）
docker exec -it llm_server ollama pull llama3.1:8b

# サービス起動
docker compose up -d

# backend 管理者ユーザー作成（Django管理者ページを使うなら）
cd backend
docker compose exec backend sh -c "python manage.py createsuperuser"
任意のユーザー名、メールアドレス、パスワードを入力してください。

# サービス停止
docker compose down
```

## 公開ポート番号

- フロントエンド: http://localhost:3333
- バックエンド: http://localhost:8000
- Django 管理者ページ: http://localhost:8000/admin

## LLM モデル例

2025 年 12 月現在のおすすめモデル一覧

```
# ローカル（Ollama 前提）

順位	モデル	コメント
- Llama 3.1 (70B)	圧倒的知識量（要高スペック）
- Llama 3.1 (8B)	実用最強バランス
- Qwen2.5	        技術・数学に強い
- Gemma 3	        軽量・Google品質
- Phi-3	            軽いが知識は少なめ

Llama 3.1 (70B) は高スペックな分、PCスペックが必要らしい。
- 容量140GB以上使用
- メモリ64GB以上推奨
- GPUのVRAMは48GB以上推奨
```

##　仕組み解説

- フロントエンド（React）からバックエンド（Django）に質問を送信
- バックエンドで Ollama API にリクエストを送信
- Ollama が LLM モデルで回答を生成し、バックエンドにレスポンスを返す
- バックエンドがフロントエンドに生成された文章を返す

## プロンプトの設定

`backend/apps/llm_config/prompts/` 以下のファイルを編集してください。

- `default.py`: これがキモ。デフォルトのシステムプロンプトを定義しています。ここを設定することで、LLM の応答スタイルを変更できます。
  　 （プロンプトをいじった場合、docker 再起動しないとダメっぽいです）

##　ファインチューニング
Ollama は現時点でファインチューニングに対応していません。
そのため、プロンプトエンジニアリングで対応する形になります。
ただし、プロンプトが長くなれば長くなるほどに、GPU やメモリ等のスペックを要求されます。
